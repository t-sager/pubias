---
title: "pubias: Identification of and Correction for Publication Bias^[This package was created as part of the seminar Workshop in Econometrics (Spring Semester 2021) and follows the implementation by *Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.*]"
abstract:
  "Following Andrews and Kasy (2019), this package implements the identification of and correction for publiaction bias. Also 
  known as the file drawer problem, the publication bias occures when researchers submit their papers based on the likelihood of their      publication. Therefore, some results have a higher publication probability than others.This package identifies the said bias and          corrects   for it by differentiating between replication and meta-studies as well as between a likelihood and GMM estimation procedure."
author: "Till Sager^[University of Bern, till.sager@students.unibe.ch]"
date: "May, 2021"
output:   
  pdf_document:
    toc: false
    toc_depth: 3  
    number_sections: true 
    latex_engine: xelatex
fontsize: 11pt
header-includes:
 \usepackage{float}
vignette: >
  %\VignetteIndexEntry{pubias: Identification of and Correction for Publication Bias^[This package was created as part of the seminar Workshop in Econometrics (Spring Semester 2021) and follows the implementation by *Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.*]"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')

```

```{r setup}

```

\centering
\raggedright
\newpage
\tableofcontents

\newpage


# Introduction

\newpage


# Publication Bias

Instead of going into detail about the theory and mathematics Andrews and Kasy (2019) used in their paper, I use an illustrative example they provided to show how a publication bias arises, how it can be identified and ultimately, how one can correct for it. 

Journals usually receive a lot of studies with different estimates and the journals need to decide which papers will be published and which are not. Let us assume that the reported estimates are normally distributed i.e. $X ~ N(Θ, Σ^2)$ where $Θ$ are the the treatment effects, and $Σ$ are the corresponding standard errors. Additionally, each study examines different treatments.
In this example, the journal publishes significant results at the 5% with probability 1 ($p(z) = 1$) and insignificant results with probability 0.2 ($p(Z) = 0.2$, with $Z ∈ [-1.96, 1.96]$). This indicate a clear preference for publishing significant results. Namely, significant result are 5 times more likely to be published than insignificant results.
Because of this behavior, it is clear that the published results over-estimate the magnitude of the treatment effect.The same holds for the confidence set. Small values are underestimates, while large values are overestimated. Although the introduction focused on the publication probability by researches, the behavior of the journals applies to the researchers as well or we can even say it applies to both at the same time. 

\newpage


# Methods and assumptions

This chapter summarizes the methods used to identify and correct for the publication bias following Andrews and Kasy (2019). In their paper, they differ between two approaches of identification. The first approach uses replication studies while the second one uses meta studies to determine the publication probability. Given a certain publication probability, they propose a method to construct the median unbiased estimators and confidence bounds.

## Identification

The procedure of identifying the publication probability differs between the two approaches. A replication study uses the *same* methods as the original study and applies them to a new sample from the *same* population. From the package perspective, we therefore need the original estimates as well as the replication estimates to determine the given publication probability. Additionally, it is assumed that the publication probability only depends on the original estimates.
On the other hand, a meta study is a study which collects multiple estimates from studies which cover more or less the same topic. One crucial assumption regarding meta studies is the independence between the different estimates. In contrast to the replication studies, meta studies only have one set of estimates as input for the estimation of the publication probability.


### Replication Studies

Again, to illustrate the identification approach using
replication studies, the illustrative example from the beginning is used.Remember, we assumed that $p(Z) = 1$ when $|Z| > 1.96$ , and that
$p(Z) = 0.2$ otherwise. A further assumption is that the original as well as the replication estimates both have a variance of 1 ($Σ^r = Σ = 1$).
Figure **3** shows how the identification using replication studies is done. The left panel shows 100 random draws $(Z, Z^r)$. Draws which are insigificant on the 5% level are marked as grey $(|Z| ≤ 1.96)$ whereas draws which are significant are drawn blue $(|Z| > 1.96)$.
The right panel shows the draws $(Z, Z^r)$ which are published. These are exactly the same draws as before but now 80% of the statistically insigfnicant results are deleted because the journal only publishes 20% of the statistically insigifcant results.
To proof that some selective publication exists, we can take a closer look at the regions $A$ and $B$. In $A$, we deal with the situation that $Z$ is statistically significant while $Z^R$ is not. Clearly, in region $B$ it is just the other way around.
Here, the assumption of the symmetry in the data generating process comes into play. Because of this assumption, $Z$ or $Z^r$ should fall in either region with the same probility. Due to the fact that we clearly observe more data points in region A than in region B, we have evidence for selective publication. That is why it is possible to estimate $p(.)$.*TODO: p() up to scale?*


```{r plot1, echo=FALSE, fig.cap="Identifcation using Replication Studies",fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_illu_rep.pdf")

```


### Meta Studies

Using the same set up as in the identification approach using replication studies and adding the crucial assumption that $Σ$ is independent of $Θ$ across the studies, we can take a closer look at Figure 2 which shows the identification approach using meta studies.
Instead of dealing with original and replication estimates and having them on the X and Y axis, we now have the original estimates $X$ on the X axis and the corresponding standard errors $Σ$ on the Y axis.
As before, the left panel shows 100 random draws $(X, Σ)$. Draws which are insignificant on the 5% level are marked as grey $(|X/Σ| ≤ 1.96)$ whereas draws which are significant are drawn blue $(|X/Σ| > 1.96)$.
The right panel shows the draws $(X,Σ)$ which are published. These are exactly the same draws as before but now 80% of the statistically insigfnicant results are deleted because the journal only publishes 20% of the statistically insigifcant results.
Intead of looking at two reagions as in the case of replication studies, we now tae a closer look at two different values of $Σ$, shown in the figure by $A$ and $B$.
Since we have assumed independence between $Σ$ and $Θ$, the distribution of $X$ given greater values of $Σ$ is an inflated version of the same distribution for smaller values of $Σ$. Thus, to the degree that the same is not true for the distribution of the published estimates $X$ given $Σ$, this has to be due to selectivity in the publication process. In this example, statistically insignificant observations are absent for greater values of $Σ$. Since the likelihood of publication is hiher if the result is significant, the estimated values $X$ tend to be larger on average for greater values of $Σ$. This allows for an estimate of $p(.)$.
*TODO: p() up to scale? + Text noch überarbeiten, letzter Teil nicht klar*


### MLE vs. GMM

Besides the maximum likelihood estimation


```{r plot2, echo=FALSE, fig.cap="Identifcation using Meta-Studies",fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_illu_meta.pdf")

```

## Correction

After computing the publication probability, Andrews and Kasy show, that one can derive corrected estimators and confidence sets. Putting it as simple as possible, the selective publication weights the distribution of $Z$, where $Z$ is equal the original estimates divided by their standard errors, by the publication probability. To obtain the corrected estimates and confidence set, one only need to correct for this weighting. 
In their approach, they define the distribution function for published results $Z$ given their *true* effect. By using procedures applied in Andrews (1993) and Stock and Watson (1998), and inverting the distribution function leads to the median unbiased estimator as well as the equal-tailed confidence sets which fully correct for the and bias produced by the selective publication. 
To get an intuition for their result, the illustrative example is continued.
Comparing the median unbiased estimator with the uncorrected estimator, we can clearly see that the median unbiased estimator lies below the original estimator, given $Z$ is small. With an increasing $Z$, we observe that the median unbiased estimator converges to the original estimator. The same observation holds for the confidence intervals. In the example from figure two, the corrected estimates are nearly equal the original estimates is $Z > 5$.

```{r plot3, echo=FALSE, fig.cap="Bias correction", fig.align="center", out.width = '80%'}

knitr::include_graphics("correction_illu.pdf")

```

\newpage

# Package Features

## Installing the Package

The package is currently available on GitHub. Running the code below will directly install the package with all its dependencies on your local machine.

```{r, eval = FALSE}

# Installing the package directly from github
devtools::install_github("t-sager/pubias")

# Load package into library
library(pubias)

```


## Syntax

There are two main functions in the package. The first `pubias_meta()` leads to the identification of and correction for the publication bias in meta studies while `pubias_replication()` does the same for replication studies. The most simple syntax to get a result is the following:

```{r, eval = FALSE}
# Meta Studies
pubias_meta(data)

# Replication Studies
pubias_replication(data)

```


Additionally, there are four functions which are, depending on their specification, called by the two functions above.  The four functions are as follows: `mle_meta()`, `mle_replication()`, `gmm_meta()` and `gmm_replication()`. For example: If the base specification for the meta studies `pubias_meta(data, studynames)` is used, then the function `mle_meta()` is called to calculate the publication probability by MLE. On the other hand, if one executes `pubias_meta(data, studynames GMM = TRUE)` the function `gmm_meta()` will be called.
Because the input for these functions heavily depend on he input from the two main function, I will not go into further detail about the arguments of these functions. One can also take a closer look at the built in helpfile with `?mle_meta()` etc.

## Arguments

We have seen that the function works even without specifying more than the data containing the estimates and the standard errors. Now we will take a closer look at the different arguments which one could enter into the function. Keep in mind: the arguments for both functions are the same. The only difference is, that in the case of replication studies, the data used has additional columns because we also have to account for the replication estimates.
To check the different arguments, one could also take a closer look at the helpfiles for the two functions with `?pubias_meta()` or `?pubias_replication()`.

The following specifications are currently possible:

```{r, eval = FALSE}

pubias_meta(
  data,
  studynames,
  symmetric = 1,
  sign_lvl = 0.05,
  GMM = FALSE,
  print_plots = FALSE,
  print_dashboard = FALSE
)

```

- `data`: In the case of meta studies, the data needs to contain a `n x 3` matrix where the first column contains the original estimates, the second column the associated standard errors and in the third column a cluster ID going from 1 to `n`, where `n` is the number of studies used in the meta-study. Keep in mind, this does not necessarily mean the same number of estimates because there could be several estimates coming from one single study.
If the function for the replication studies is used, the data needs to contain a `n x 4` matrix where the first (third) column contains the standardized original estimates (replication estimates) and the second (fourth) column the associated standard errors, where `n` is the number of estimates.

- `studynames`: This argument is optional. A vector of type character containing all the Studynames of size n in the same order as the argument data.

- `symmetric`: If set to 1, the publication probability is assumed to be symmetric around zero. If set to 0, asymmetry is allowed. !IN THE CURRENT BUILD, THIS IS NOT WORKING!

- `sign_lvl`: A value indicating the significance level at which the analysis should be done. Ultimately leads to the threshold (z-score) for the steps of the publication probability. By default, the significance level is set at `5%`, hence `0.05`.

- `GMM`: If set to `TRUE`, the publication probability will be estimated via GMM. By default, it is set to `FALSE` which uses the MLE method for estimation.

- `print_plots`: If set to `TRUE`, descriptive plots as well as correction plots are printed into the working directory in .pdf format.

- `print_dashboard`: If set to `TRUE`, additionally to the .pdf plots, a dashboard with the same charts in dynamic format is produced. The dashboard is saved in the working directory. Only possible if `print_plots` is set to `TRUE`.


## Results

The output of the functions is pretty simple. They return a list object called `pubias_results`. If only the base specification is run, the list contains two elements:

- `Results`: This element contains the three results `Psihat` which is the estimated publication probability, `Varhat` its variance and`se_robust` containing the robust standard errors for the publication probability. For interpretation: If the function yields `Psihat = 0.25`, the publication of insignificant results is only 25% as likely as the publication of significant results. Or put differently, the publication of a significant result is 4 times more likely than an insignificant result (at the given significance level).

- `Corrected Estimates`:  This element reports the result from the publication bias correction. First, it contains the original estimates with their 95% confidence bounds (`Z1`, `Z1_L`, `Z1_U`) as well as the corrected estimates (`Z1_M`) and in addition the Bonferroni corrected 95% confidence bounds (`Z1_LB`, `Z1_UB`). There are additional elements which are mainly used for plotting the results.

If the functions are run with the option to print the plots and the dashboard, the result list also contains the `ggplot` objects for the descriptive as well as the correction plots. The plots will additionally be printed as .pdf into the current working directory. If specified, a HTML-Dashboard with dynamic plots will be exported as well. One can also look at the plots by calling: 

```{r, eval = FALSE}

# Display the scatterplot
pubias_result$`Descriptive Plots`$scatter

# Display the correction plot
pubias_result$`Correction Plots`$CorrectionPlot

```

\newpage

# Applications

To further illustrate the functionality of the package, we apply it to a replication study as well as a meta analysis. These are the same examples used by Andrews and Kasy (2019).

## Replication Study: Economics Laboratory Experiments

This application uses data from a recent study by Camerer et al. (2016). They replicated 18
economic laboratory experiment papers which were published between 2011 and 2014. The data is well suited for this application because it replicates estimates from papers which come from the same area of research and are all published in
well known journals. Additionally, it seems likely that Camerer et al. would have published their results regardless of their significance. This is consistent with the assumption stated at the beginning: publication selection only arises based on the original estimates and not on the replication estimates.
Ultimately, the dataset includes the original and replication estimates with their associated standard errors. This leads to a matrix with the dimensions 18 by 4.

Because the datasets for the applications are part of the package itself, it is really easy to replicate the results. The code below shows how it is done. Of course, one can load own estimates and studynames via `read.csv()` or similar functions. 
```{r, eval = FALSE}

# Import data
data <- data_econ

# Has to be matrix
class(data)
dim(data) # 18x4 matrix

# Import Studynames
studynames <- studynames_econ

# Has to be character
class(studynames)

# Apply the main function
pubias_replication(
  data,
  studynames,
  GMM = FALSE,
  symmetric = 1,
  sign_lvl = 0.05,
  print_plots = TRUE,
  print_dashboard = TRUE
)

```

After running the function, we can take a closer look at the results. The following figures correspond to the figures which are printed if `print_plots = TRUE`. Before turning to the estimation result, let us take a closer look at the descriptives.
The histogram on the left side of Figure 4 shows the distribution of the original published estimates. There is a considerable jump in the density right around the z-score of 1.96 which corresponds to the significance level of 5%. As Andrews and Kasy (2019) show in their proofs, this directly correspond to a jump in the publication probability at the same cutoff. On the right hand side of Figure 4, we see the same type of plot as described in part 3.1.1. Following this interpretation, we can clearly see some evidence for publication selectivity.

```{r plot4, echo=FALSE, fig.cap="Economics Laboratory Experiments: Descriptives", fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_hist_econ.pdf")

```

The MLE estimation leads to a publication probability of about 2.8%. This means the likelihood of publication is much lower if the the results are insignificant. In other word: significant result ar more than 30 times more likely to be published than insignificant results at the 5% significance level. This shows that the publication bias could lead to substantial differences in published result.

That is why the next step is to correct for the said bias based on the publication probability of 2.8%. The reults are summarized in the following two plots. Figure 5 plots the original, adjusted as well as the replication estimates. One can see that the adjusted estimates follow the replication estimates pretty well and are much smaller than the original estimates most of the time. Looking at the confidence sets of the adjusted and original estimates, we can see clear difference. While only 2 of the original estimates confidence bounds included 0, with adjusting for the publication this number goes up to 12. Therefore, the adjustment leads to substantially less significant results. 
Figure 6 corresponds to the figure in chapter 3.2 but we additionally plot the Bonferroni corrected confidence bounds. The interpretation stays the same. The corrected estimates lie clearly below the original estimate with the furthest distance exactly at our chosen cutoff. Again, the difference shrinks as the original estimates get larger or smaller.

```{r plot5, echo=FALSE, fig.cap="Economics Laboratory Experiments: Original vs. Adjusted Estimates", fig.align="center", out.width = '70%'}

knitr::include_graphics("org_adj_rep_econ.pdf")

```


```{r plot6, echo=FALSE, fig.cap="Economics Laboratory Experiments: Correction Plot", fig.align="center", out.width = '70%'}

knitr::include_graphics("correction_plot_econ.pdf")

```


## Meta Analysis: Deworming

The second application uses a meta-study by Croke et al. (2016). They look at the effect of a drug for deworming on the body weight of children. The authors collect a total of 22 estimates from 20 different studies. These 22 estimates together with their associated standard errors build the main data input for our function `pubias_meta()`. Because there is a possibility of multiple estimates for some of the studies, one has to cluster for the study ID. This is why the study-ID has to enter the input data as well. Contrary to the first application, we use the GMM approach in the second 
As stated before, the data for the two documented applications can be loaded directly from the package as shown below. Of course, alternatively, one can load own estimates and studynames via `read.csv()` or similar functions.

```{r, eval = FALSE}

# Import data
data <- data_dew

# Has to be matrix
class(data)
dim(data) # 22x3 matrix

# Import Studynames
studynames <- studynames_dew

# Has to be character
class(studynames)

# Apply the main function
pubias_meta(
  data,
  studynames,
  GMM = TRUE,
  symmetric = 1,
  sign_lvl = 0.05,
  print_plots = TRUE,
  print_dashboard = TRUE
)

```

As in the application for replication studies, we first take a look at the descriptives. The histogram shows the density of the standardized estimtates (i.e. $X/\Sigma$). We see a clear jump in the density around 0 which suggest evidence for selection for *positive* estimates. On the right side of Figure 8 we see a similar plot to as in chapter 3.1.2. Applying the same logic, we do not clearly see evidence for selective probability, i.e. there is no clear positive correlation between $X$ and $\Sigma$. 

```{r plot7, echo=FALSE, fig.cap="Deworming: Descriptives", fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_hist_dew.pdf")

```

Looking at the estimation results indicates a publication probability of about 25%, This is suggesting that statistically significant results are 4 times more likely to be included in the meta-study by Croke et al. (2016). Allthough this result seems decisive, Andrews and Kasy (2019) calculate different specifications for this application and show, that the results can differ a lot. They even get a result, suggesting that insignificant results are more likely to be published than significant ones. Due to their different specifications, we can not draw any final conclusions about the selective publication.
Let us still take a quick look at the correction plots. In Figure 8 we compare the adjusted with the original estimates. It is obvious that estimates closer to the cutoff of 1.96 are adjusted quite a bit more than the other estimates. Generally, the adjusted estimates are drawn closer to zero, which would also mean that with the adjustment, we get less significant results.
Again, in Figure 9, we can see the biggest difference between the unbiased estimates and the original estimates around the cutoff 1.96. Above an estimation of 4, the adjusted equal the original estimates. The same holds for the estimates close to 0.

```{r plot8, echo=FALSE, fig.cap="Deworming: Original vs. Adjusted Estimates", fig.align="center", out.width = '70%'}

knitr::include_graphics("org_adj_dew.pdf")

```


```{r plot9, echo=FALSE, fig.cap="Deworming: Correction Plot", fig.align="center", out.width = '70%'}

knitr::include_graphics("correction_plot_dew.pdf")

```

# Final remarks


# Literature

- Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.
- Croke et al. (2016)
- Camerer et al. (2016)
