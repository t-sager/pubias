---
title: "pubias: Identification of and Correction for Publication Bias^[This package was created as part of the seminar Workshop in Econometrics (Spring Semester 2021) and follows the implementation by *Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.*]"
abstract:
  "Following Andrews and Kasy (2019), this package implements the identification of and correction for publiaction bias. Also 
  known as the file drawer problem, the publication bias occures when researchers submit their papers based on the likelihood of their publication. Therefore, some results have a higher publication probability than others.This package identifies the said bias and corrects for it by differentiating between replication and meta-studies as well as between a likelihood and GMM estimation procedure."
author: "Till Sager^[University of Bern, till.sager@students.unibe.ch]"
date: "May, 2021"
output:   
  pdf_document:
    toc: false
    toc_depth: 3  
    number_sections: true 
    latex_engine: xelatex
fontsize: 11pt
header-includes:
 \usepackage{float}
vignette: >
  %\VignetteIndexEntry{pubias: Identification of and Correction for Publication Bias^[This package was created as part of the seminar Workshop in Econometrics (Spring Semester 2021) and follows the implementation by *Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.*]"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')

```

```{r setup}

```

\newpage
\tableofcontents

\newpage


# Introduction

Research can be a highly competitive environment regarding funding or reaching the next step in ones career. It is plausible that researches are under pressure and have an incentive to only submit results which favor them and are more likely to be published in famous journals. The journal editors themselves may also deal with an incentive to publish more positive findings because they have to compete with other journals for more citations and have to be financed as well.
In combination, this may lead to the so called file drawer problem (Rosenthal 1979), selective publication or publication bias. This bias does not only occur in the field of economics, it is a wide spread problem which also concern life sciences, medicine and health care research and other social sciences. Depending on the field of interest, holding back negative results can lead to the distortion of the literature, may have consequences for the health of the citizens, using scarce resources for wrong investments or just have a negative impact on research an teaching practices. 
Fanelli (2011) shows in a study looking at 4600 papers published between 1990 and 2007 that negative results are disappearing from being published and his research suggests an increase in publication bias over these years. According to him, papers finding significant support for their initial hypotheses increased by 22% in this timespan. By the ethic code of researchers and journal editors, they are bound to publish both negative as well as positive results in a non selective manner. Therefore, one might argue that this decrease in negative results is just the result of getting better at formulating hypotheses. Concerning the competition researchers and editors find themselves in, this may lead to the exclusion of poorly formulated hypothesis and experiments which ultimately leads to an increase in positive results (Joober et al, 2012). On the other hand, replicating existing results often lead to completely different estimates with smaller magnitude or even opposite signs which would no support this hypothesis.
The publication bias is most certainly a topic which needs to be addressed. There are several methods to tackle the problem. The documented R package implements the method documented by Andrews and Kasy (2019).

\newpage


# Publication Bias

Instead of going into detail about the theory and mathematics Andrews and Kasy (2019) used in their paper, I use an illustrative example they provided to show how a publication bias arises, how it can be identified and ultimately, how one can correct for it. 

To get a feeling for how a publication bias can arise, we look at the behavior of journals which are responsible for publishing the papers they are provided with. Journals usually receive a lot of studies with different outcomes and they need to decide which papers will be published and which are not. Let us assume that the reported estimates $X$ are normally distributed i.e. $X ~ N(Θ, Σ^2)$ where $Θ$ are the true treatment effects, and $Σ$ are the corresponding standard errors. $Z$ denotes the standardized estimates given by $X/\Sigma$. Additionally, each study examines different treatments.

In this example, the journal publishes significant results at the 5% significance level with probability 1 ($p(z) = 1$) and insignificant results with probability 0.2 ($p(Z) = 0.2$, with $Z ∈ [-1.96, 1.96]$). This indicates a clear preference for publishing significant results. Namely, significant result are 5 times more likely to be published than insignificant results.
Because of this behavior, it is clear that the published results $X$ over-estimate the magnitude of the treatment effect $Θ$.The same holds for the confidence bounds. Small values are underestimated, while large values are overestimated. 
Keep in mind that publication decisions are based on both the researchers as well as the journals decision. Andrews and Kasy do not try to separate those two decision dimensions but rather look at them as one decision only. Therefore, the above example also holds for the researchers or even for the journals an the researchers at the same time.

\newpage


# Methods and assumptions

This chapter summarizes the methods used to identify and correct for the publication bias following the approach by Andrews and Kasy (2019). In their paper, they differ between two approaches of identification. The first approach uses replication studies while the second one uses meta studies to determine the publication probability. Given a certain publication probability, they propose a method to construct the median unbiased estimators (i.e. the corrected estimates) and confidence bounds.

## Identification

The procedure of identifying the publication probability differs between the two approaches. A replication study uses the *same* methods as the original study and applies them to a new sample from the *same* population. From the package perspective, we therefore need the original estimates as well as the replication estimates to determine the given publication probability. Additionally, it is assumed that the publication probability only depends on the original estimates.

On the other hand, a meta study is a study which collects multiple estimates from studies which cover more or less the same topic. One crucial assumption regarding meta studies is the independence between the different estimates. In contrast to the replication studies, meta studies only have one set of estimates as input for the estimation of the publication probability.


### Replication Studies

To illustrate the identification approach using replication studies, the illustrative example from the beginning is used. Remember, we assumed that $p(Z) = 1$ when $|Z| > 1.96$ , and that $p(Z) = 0.2$ otherwise. A further simplification we have to take into account is that the original as well as the replication estimates both have the same variance of 1 ($Σ^r = Σ = 1$).
Figure 1 shows how the identification using replication studies is done. The left panel shows 100 random draws $(Z, Z^r)$, where $Z^2$ denotes the standradized replication estimates (i.e. $X^r/\Sigma^r$). Draws which are insignificant on the 5% level are marked as light blue $(|Z| ≤ 1.96)$ whereas draws which are significant are drawn in a darker blue $(|Z| > 1.96)$.

The right panel shows the draws $(Z, Z^r)$ which are published. These are exactly the same draws as before but now 80% of the statistically insignificant results are deleted because the journal only publishes 20% of the statistically insignificant results. That is why a lot of the light blue points vanish.
To proof that some selective publication exists, we can take a closer look at the regions $A$ and $B$. In $A$, we deal with the situation that $Z$ is statistically significant while $Z^R$ is not. Obviously, we deal with the exact opposite in region $B$.
At this point we have to mention that we assumed symmetry in the data generating process at the beginning. Because of this assumption, $Z$ or $Z^r$ should fall in either region with the same probability. Due to the fact that we clearly observe more data points in region A than in region B, we have evidence for selective publication. Additionally, the deviation from symmetry ($Z = Z^r$) exactly identifies $p(.)$.


```{r plot1, echo=FALSE, fig.cap="Identifcation using Replication Studies",fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_illu_rep.pdf")

```


### Meta Studies

Using the same set up as in the identification approach using replication studies and adding the crucial assumption that $Σ$ is independent of $Θ$ across the studies, we can take a closer look at Figure 2 which shows the identification approach using meta studies.
Instead of dealing with original and replication estimates and having them on the X and Y axis, we now have the original estimates $X$ on the X -xis and the corresponding standard errors $Σ$ on the Y-axis. As before, the left panel shows 100 random draws $(X, Σ)$. Draws which are insignificant on the 5% level are marked as light blue $(|X/Σ| ≤ 1.96)$ whereas draws which are significant are drawn in a darker blue $(|X/Σ| > 1.96)$. The right panel shows the draws $(X,Σ)$ which are published. Again, these are exactly the same draws as in the left panel but now 80% of the statistically insignificant results are deleted because the journal only publishes 20% of the statistically insignificant results.
Instead of looking at two regions as in the case of replication studies, we now take a closer look at two different values of $Σ$. These two values are at $Σ = 0.5$ and $Σ = 1.0$, highlighted by $A$ and $B$.
Since we have assumed independence between $Σ$ and $Θ$, the distribution of $X$ given greater values of $Σ$ is an inflated version of the same distribution for smaller values of $Σ$. Thus, to the degree that the same is not true for the distribution of the published estimates $X$ given $Σ$, this has to be due to selectivity in the publication process. In this example, statistically insignificant observations are absent for greater values of $Σ$. In other words, some of the darker blue points on the top right should shift in reality to the left, below the threshold of 1.96. Since the likelihood of publication is higher if the result is significant, the estimated values $X$ tend to be larger on average for greater values of $Σ$. Therefore, we expect some positive correlation between the two. Given the conditional distribution of $X$ given $Σ$ changes with $Σ$, we are again able to identify the publication probability.

```{r plot2, echo=FALSE, fig.cap="Identifcation using Meta-Studies",fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_illu_meta.pdf")

```

### MLE vs. GMM

The maximum likelihood estimation is the basic estimation procedure reported by Andrews and Kasy (2019). Because this approach heavily relies on the parametric assumptions on the distribution of the true treatment effects $Θ$, they provide a generalized method of moments estimator in their appendix. The advantage of GMM is, that the parametric assumptions can be dropped and one only has to assume some functional form for the publication probability. They do this to show some robustness of their results. Meaning, that even without assuming parametric specifications, similar results can be obtained. Their result confirm this. Although moment-based methods lead to less precise results, their main findings still hold. Because it seems that the GMM estimation is a good alternative to the likelihood estimation, both approaches wre implemented in the package. For further information on their method of moments approach, please check the appendix in Andrews and Kasy (2019)

## Correction

After computing the publication probability, Andrews and Kasy show that one can derive corrected estimators and confidence sets. Putting it as simple as possible, the selective publication weights the distribution of $Z$ by the publication probability. To obtain the corrected estimates and confidence sets, one only needs to correct for this weighting.

In their approach, they define the distribution function for published standardized results $Z$ given their *true* effect. By using procedures applied in Andrews (1993) and Stock and Watson (1998), and inverting the distribution function, they are able to calculate the median unbiased estimator (i.e. corrected estimates) as well as the equal-tailed confidence sets which fully correct for the bias produced by the selective publication. 
To get an intuition for their result, the illustrative example is continued. Comparing the median unbiased estimator with the original and uncorrected estimates (grey line), we can clearly see that the median unbiased estimator lies below the original estimator, given $Z$ is close to the cutoff 1.96. With an increasing (decreasing) $Z$, we observe that the median unbiased estimator converges to the original estimator. The same observation holds for the confidence intervals. In the example from Figure, the corrected estimates are nearly equal the original estimates if $Z > 5$.

```{r plot3, echo=FALSE, fig.cap="Bias correction", fig.align="center", out.width = '80%'}

knitr::include_graphics("correction_illu.pdf")

```

\newpage

# Package Features

## Installing the Package

The package is currently available on GitHub. Running the code below will directly install the package with all its dependencies on your local machine.

```{r, eval = FALSE}

# Installing the package directly from github
devtools::install_github("t-sager/pubias")

# If the package vignette should be downloaded as well
devtools::install_github("t-sager/pubias", build_vignettes = TRUE)
# Check vignette
vignette("pubias")

# Load package into library
library(pubias)

```


## Syntax

There are two main functions in the package. The first `pubias_meta()` leads to the identification of and correction for the publication bias in meta studies while `pubias_replication()` does the same for replication studies. The most simple syntax to get a result is the following:

```{r, eval = FALSE}
# Meta Studies
pubias_meta(data)

# Replication Studies
pubias_replication(data)

```


Additionally, there are four functions which are, depending on their specification, called by the two functions above.  The four functions are as follows: `mle_meta()`, `mle_replication()`, `gmm_meta()` and `gmm_replication()`. For example: If the base specification for the meta studies `pubias_meta(data, studynames)` is used, then the function `mle_meta()` is called to calculate the publication probability by MLE. On the other hand, if one executes `pubias_meta(data, studynames GMM = TRUE)` the function `gmm_meta()` will be called.
Because the input for these functions heavily depend on he input from the two main function, I will not go into further detail about the arguments of these functions. One can also take a closer look at the built in helpfile with `?mle_meta()` etc.

## Arguments

We have seen that the function works even without specifying more than the data containing the estimates and the standard errors. Now we will take a closer look at the different arguments which one could enter into the function. Keep in mind: the arguments for both functions are the same. The only difference is, that in the case of replication studies, the data used has additional columns because we also have to account for the replication estimates.
To check the different arguments, one could also take a closer look at the helpfiles for the two functions with `?pubias_meta()` or `?pubias_replication()`.

The following specifications are currently possible:

```{r, eval = FALSE}

pubias_meta(
  data,
  studynames,
  symmetric = TRUE,
  sign_lvl = 0.05,
  GMM = FALSE,
  print_plots = FALSE,
  print_dashboard = FALSE
)

```

- `data`: In the case of meta studies, the data needs to contain a `n x 3` matrix where the first column contains the original estimates, the second column the associated standard errors and in the third column a cluster ID going from 1 to `n`, where `n` is the number of studies used in the meta-study. Keep in mind, this does not necessarily mean the same number of estimates because there could be several estimates coming from one single study.
If the function for the replication studies is used, the data needs to contain a `n x 4` matrix where the first (third) column contains the standardized original estimates (replication estimates) and the second (fourth) column the associated standard errors, where `n` is the number of estimates.

- `studynames`: This argument is optional. A vector of type character containing all the Studynames of size n in the same order as the argument data.

- `symmetric`: If set to `TRUE`, the publication probability is assumed to be symmetric around zero (default). If set to `FALSE`, asymmetry is allowed. 

- `sign_lvl`: A value indicating the significance level at which the analysis should be done. Ultimately leads to the threshold (z-score) for the steps of the publication probability. By default, the significance level is set at `5%`, hence `0.05`.

- `GMM`: If set to `TRUE`, the publication probability will be estimated via GMM. By default, it is set to `FALSE` which uses the MLE method for estimation.

- `print_plots`: If set to `TRUE`, descriptive plots as well as correction plots are printed into the working directory in .pdf format.

- `print_dashboard`: If set to `TRUE`, additionally to the .pdf plots, a dashboard with the same charts in dynamic format is produced. The dashboard is saved in the working directory. Only possible if `print_plots` is set to `TRUE`.


## Results

The output of the functions is pretty simple. They return a list object called `pubias_results`. If only the base specification is run, the list contains two elements:

- `Results`: This element contains the three results `Psihat` which is the estimated publication probability, `Varhat` its variance and`se_robust` containing the robust standard errors for the publication probability. For interpretation: If the function yields `Psihat = 0.25`, the publication of insignificant results is only 25% as likely as the publication of significant results. Or put differently, the publication of a significant result is 4 times more likely than an insignificant result (at the given significance level).

- `Corrected Estimates`:  This element reports the result from the publication bias correction. First, it contains the original estimates with their 95% confidence bounds (`Z1`, `Z1_L`, `Z1_U`) as well as the corrected estimates (`Z1_M`) and in addition the Bonferroni corrected 95% confidence bounds (`Z1_LB`, `Z1_UB`). There are additional elements which are mainly used for plotting the results.

If the functions are run with the option to print the plots and the dashboard, the result list also contains the `ggplot` objects for the descriptive as well as the correction plots. The plots will additionally be printed as .pdf into the current working directory. If specified, a HTML-Dashboard with dynamic plots will be exported as well. One can also look at the plots by calling: 

```{r, eval = FALSE}

# Display the scatterplot
pubias_result$`Descriptive Plots`$scatter

# Display the correction plot
pubias_result$`Correction Plots`$CorrectionPlot

```

\newpage

# Applications

To further illustrate the functionality of the package, we apply it to a replication study as well as a meta analysis. These are the same examples used by Andrews and Kasy (2019).

## Replication Study: Economics Laboratory Experiments

This application uses data from a recent study by Camerer et al. (2016). They replicated 18
economic laboratory experiment papers which were published between 2011 and 2014. The data is well suited for this application because it replicates estimates from papers which come from the same area of research and are all published in
well known journals. Additionally, it seems likely that Camerer et al. would have published their results regardless of their significance. This is consistent with the assumption stated at the beginning: publication selection only arises based on the original estimates and not on the replication estimates.
Ultimately, the dataset includes the original and replication estimates with their associated standard errors. This leads to a matrix with the dimensions 18 by 4.

Because the datasets for the applications are part of the package itself, it is really easy to replicate the results. The code below shows how it is done. Of course, one can load own estimates and studynames via `read.csv()` or similar functions. 
```{r, eval = FALSE}

# Import data
data <- data_econ

# Has to be matrix
class(data)
dim(data) # 18x4 matrix

# Import Studynames
studynames <- studynames_econ

# Has to be character
class(studynames)

# Apply the main function
pubias_replication(
  data,
  studynames,
  GMM = FALSE,
  symmetric = TRUE,
  sign_lvl = 0.05,
  print_plots = TRUE,
  print_dashboard = TRUE
)

```

After running the function, we can take a closer look at the results. The following figures correspond to the figures which are printed if `print_plots = TRUE`. Before turning to the estimation result, let us take a closer look at the descriptives.
The histogram on the left side of Figure 4 shows the distribution of the original published estimates. There is a considerable jump in the density right around the z-score of 1.96 which corresponds to the significance level of 5%. As Andrews and Kasy (2019) show in their proofs, this directly correspond to a jump in the publication probability at the same cutoff. On the right hand side of Figure 4, we see the same type of plot as described in part 3.1.1. Following this interpretation, we can clearly see some evidence for publication selectivity.

```{r plot4, echo=FALSE, fig.cap="Economics Laboratory Experiments: Descriptives", fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_hist_econ.pdf")

```

The MLE estimation leads to a publication probability of about 2.8%. This means the likelihood of publication is much lower if the the results are insignificant. In other word: significant result ar more than 30 times more likely to be published than insignificant results at the 5% significance level. This shows that the publication bias could lead to substantial differences in published result.

That is why the next step is to correct for the said bias based on the publication probability of 2.8%. The reults are summarized in the following two plots. Figure 5 plots the original, adjusted as well as the replication estimates. One can see that the adjusted estimates follow the replication estimates pretty well and are much smaller than the original estimates most of the time. Looking at the confidence sets of the adjusted and original estimates, we can see clear difference. While only 2 of the original estimates confidence bounds included 0, with adjusting for the publication this number goes up to 12. Therefore, the adjustment leads to substantially less significant results. 
Figure 6 corresponds to the figure in chapter 3.2 but we additionally plot the Bonferroni corrected confidence bounds. The interpretation stays the same. The corrected estimates lie clearly below the original estimate with the furthest distance exactly at our chosen cutoff. Again, the difference shrinks as the original estimates get larger or smaller.

```{r plot5, echo=FALSE, fig.cap="Economics Laboratory Experiments: Original vs. Adjusted Estimates", fig.align="center", out.width = '70%'}

knitr::include_graphics("org_adj_rep_econ.pdf")

```


```{r plot6, echo=FALSE, fig.cap="Economics Laboratory Experiments: Correction Plot", fig.align="center", out.width = '70%'}

knitr::include_graphics("correction_plot_econ.pdf")

```


## Meta Analysis: Deworming

The second application uses a meta-study by Croke et al. (2016). They look at the effect of a drug for deworming on the body weight of children. The authors collect a total of 22 estimates from 20 different studies. These 22 estimates together with their associated standard errors build the main data input for our function `pubias_meta()`. Because there is a possibility of multiple estimates for some of the studies, one has to cluster for the study ID. This is why the study-ID has to enter the input data as well. Contrary to the first application, we use the GMM approach in the second 
As stated before, the data for the two documented applications can be loaded directly from the package as shown below. Of course, alternatively, one can load own estimates and studynames via `read.csv()` or similar functions.

```{r, eval = FALSE}

# Import data
data <- data_dew

# Has to be matrix
class(data)
dim(data) # 22x3 matrix

# Import Studynames
studynames <- studynames_dew

# Has to be character
class(studynames)

# Apply the main function
pubias_meta(
  data,
  studynames,
  GMM = TRUE,
  symmetric = TRUE,
  sign_lvl = 0.05,
  print_plots = TRUE,
  print_dashboard = TRUE
)

```

As in the application for replication studies, we first take a look at the descriptives. The histogram shows the density of the standardized estimates (i.e. $X/\Sigma$). We see a clear jump in the density around 0 which suggest evidence for selection for *positive* estimates. On the right side of Figure 8 we see a similar plot to as in chapter 3.1.2. Applying the same logic, we do not clearly see evidence for selective probability, i.e. there is no clear positive correlation between $X$ and $\Sigma$. 

```{r plot7, echo=FALSE, fig.cap="Deworming: Descriptives", fig.align="center", out.width = '70%'}

knitr::include_graphics("scatter_hist_dew.pdf")

```

Looking at the estimation results indicates a publication probability of about 25%, This is suggesting that statistically significant results are 4 times more likely to be included in the meta-study by Croke et al. (2016). Although this result seems decisive, Andrews and Kasy (2019) calculate different specifications for this application and show, that the results can differ a lot. They even get a result, suggesting that insignificant results are more likely to be published than significant ones. Due to their different specifications, we can not draw any final conclusions about the selective publication.
Let us still take a quick look at the correction plots. In Figure 8 we compare the adjusted with the original estimates. It is obvious that estimates closer to the cutoff of 1.96 are adjusted quite a bit more than the other estimates. Generally, the adjusted estimates are drawn closer to zero, which would also mean that with the adjustment, we get less significant results.
Again, in Figure 9, we can see the biggest difference between the unbiased estimates and the original estimates around the cutoff 1.96. Above an estimation of 4, the adjusted equal the original estimates. The same holds for the estimates close to 0.

```{r plot8, echo=FALSE, fig.cap="Deworming: Original vs. Adjusted Estimates", fig.align="center", out.width = '70%'}

knitr::include_graphics("org_adj_dew.pdf")

```


```{r plot9, echo=FALSE, fig.cap="Deworming: Correction Plot", fig.align="center", out.width = '70%'}

knitr::include_graphics("correction_plot_dew.pdf")

```

\newpage

# Final remarks

Currently, the package covers the base specification of the procedure proposed by Andrews and Kasy. In their paper, they propose further specifications like allowing for more than one cutoff, differ between numerical and analytic integration or even control for certain journals the papers were published in. These extensions could be added to the package in the future. In its first version, the aim was to build the package as simple as possible. The user should be able to just input a simple data matrix and get a reasonable result which can be easily interpreted. After conducting meta analyses or replicating a lot of different studies, there is not much time left to check for something like the publication bias. This package provides an easy to use and quick way to do some robustness checks.
Besides the contents of package, there are obviously alternative approaches to dealing with the publication bias. Andrews and Kasy discuss some of them in their paper. One of the most popular techniques are the so called meta-regression introduced by Card and Krueger (1995) and Egger et al. (1997) which regress the effect size of a study on characteristics of the study to explain the heterogenity of treatment effects between different studies and therefore try to identify a publication bias. 
Andres and Kasy also touch on the topics of manipulation or p-Hacking and discuss the question if results should even replicate or if it is completely normal if they deviate from the original estimates. In conclusion, the procedure implemented in the package is by far not the only method of dealing with a publication bias. The problem was identified a long time ago and there is still a lot of ongoing research regarding this topic.

\newpage

# Refrences

* **Andrews, Donald W. K.** 1993. “Exactly Median-Unbiased Estimation of First Order Autoregressive/
Unit Root Models.” Econometrica 61 (1): 139–65.

* **Andrews, Isaiah, and Maximilian Kasy.** 2019. "Identification of and Correction for Publication Bias." American Economic Review, 109 (8): 2766-94.

* **Card, David, and Alan B. Krueger.** 1995. “Time-Series Minimum-Wage Studies: A Meta-Analysis.” American Economic Review 85 (2): 238–43.

* **Croke, Kevin, Joan Hamory Hicks, Eric Hsu, Michael Kremer, and Edward Miguel.** 2016. “Does Mass Deworming Affect Child Nutrition? Meta-Analysis, Cost-Effectiveness, and Statistical Power.” NBER Working Paper 22382.

* **Camerer, Colin F., Anna Dreber, Eskil Forsell, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al.** 2016. “Evaluating Replicability of Laboratory Experiments in Economics.” Science 351 (6280): 1433–36.

* **Egger, Matthias, George Davey Smith, Martin Schneider, and Christoph Minder.** 1997. “Bias in Meta-Analysis Detected by a Simple, Graphical Test.” BMJ 315: 629–34.

* **Fanelli D.** 2012 "Negative results are disappearing from most disciplines and countries." Scientometrics. 90:891–904.

* **Joober R, Schmitz N, Annable L, Boksa P.** 2012. Publication bias: what are the challenges and can they be overcome?. J Psychiatry Neurosci. 37(3):149-152. 

* **Rosenthal, Robert.** 1979. “The File Drawer Problem and Tolerance for Null Results.” Psychological Bulletin 86 (3): 638–41.

* **Stock, James H., and Mark W. Watson.** 1998. “Median Unbiased Estimation of Coefficient Variance in a Time-Varying Parameter Model.” Journal of the American Statistical Association 93 (441): 349–58.
