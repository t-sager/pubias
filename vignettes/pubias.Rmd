---
title: "pubias: Identification of and Correction for Publication Bias^[This package was created as part of the seminar Workshop in Econometrics (Spring Semester 2021) and follows the implementation by *Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.*]"
abstract:
  "Following Andrews and Kasy (2019), this package implements the identification of and correction for publiaction bias. Also 
  known as the file drawer problem, the publication bias occures when researchers submit their papers based on the likelihood of their      publication. Therefore, some results have a higher publication probability than others.This package identifies the said bias and          corrects   for it by differentiating between replication and meta-studies as well as between a likelihood and GMM estimation procedure."
author: "Till Sager^[University of Bern, till.sager@students.unibe.ch]"
date: "May, 2021"
output:   
  pdf_document:
    toc: false
    toc_depth: 3  
    number_sections: true 
    latex_engine: xelatex
fontsize: 11pt
header-includes:
 \usepackage{float}
vignette: >
  %\VignetteIndexEntry{pubias: Identification of and Correction for Publication Bias^[This package was created as part of the seminar Workshop in Econometrics (Spring Semester 2021) and follows the implementation by *Andrews, Isaiah, and Maximilian Kasy. 2019. Identification of and Correction for Publication Bias.*]"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(fig.pos = 'H')

```

```{r setup}

```

\centering
\raggedright
\newpage
\tableofcontents

\newpage


# Introduction

# Publication Bias: Illustrative Example

Instead of going into detail about the theory and mathematics Andrews and Kasy (2019) used in their paper, I use an illustrative example they provided to show how a publication bias arises, how it can be identified and ultimately, how one can correct for it. 

Journals usually receive a lot of studies with different estimates and the journals need to decide which papers will be published and which are not. Let us assume that the reported estimates are normally distributed i.e. $X ~ N(Θ, Σ^2)$ where $Θ$ are the the treatment effects, and $Σ$ are the corresponding standard errors. Additionally, each study examines different treatments.
In this example, the journal publishes significant results at the 5% with probability 1 ($p(z) = 1$) and insignificant results with probability 0.2 ($p(Z) = 0.2$, with $Z ∈ [-1.96, 1.96]$). This indicate a clear preference for publishing significant results. Namely, significant result are 5 times more likely to be published than insignificant results.
Because of this behavior, it is clear that the published results over-estimate the magnitude of the treatment effect.The same holds for the confidence set. Small values are underestimates, while large values are overestimated. Although the introduction focused on the publication probability by researches, the behavior of the journals applies to the researchers as well or we can even say it applies to both at the same time. 


# Methods and assumptions

This chapter summarizes the methods used to identify and correct for the publication bias following Andrews and Kasy (2019). In their paper, they differ between two approaches of identification. The first approach uses replication studies while the second one uses meta studies to determine the publication probability. Given a certain publication probability, they propose a method to construct the median unbiased estimators and confidence bounds.

## Identification

The procedure of identifying the publication probability differs between the two approaches. A replication study uses the *same* methods as the original study and applies them to a new sample from the *same* population. From the package perspective, we therefore need the original estimates as well as the replication estimates to determine the given publication probability. Additionally, it is assumed that the publication probability only depends on the original estimates.
On the other hand, a meta study is a study which collects multiple estimates from studies which cover more or less the same topic. One crucial assumption regarding meta studies is the independence between the different estimates. In contrast to the replication studies, meta studies only have one set of estimates as input for the estimation of the publication probability.


### Replication Studies

Again, to illustrate the identification approach using
replication studies, the illustrative example from the beginning is used.Remember, we assumed that $p(Z) = 1$ when $|Z| > 1.96$ , and that
$p(Z) = 0.2$ otherwise. A further assumption is that the original as well as the replication estimates both have a variance of 1 ($Σ^r = Σ = 1$).
Figure **3** shows how the identification using replication studies is done. The left panel shows 100 random draws $(Z, Z^r)$. Draws which are insigificant on the 5% level are marked as grey $(|Z| ≤ 1.96)$ whereas draws which are significant are drawn blue $(|Z| > 1.96)$.
The right panel shows the draws $(Z, Z^r)$ which are published. These are exactly the same draws as before but now 80% of the statistically insigfnicant results are deleted because the journal only publishes 20% of the statistically insigifcant results.
To proof that some selective publication exists, we can take a closer look at the regions $A$ and $B$. In $A$, we deal with the situation that $Z$ is statistically significant while $Z^R$ is not. Clearly, in region $B$ it is just the other way around.
Here, the assumption of the symmetry in the data generating process comes into play. Because of this assumption, $Z$ or $Z^r$ should fall in either region with the same probility. Due to the fact that we clearly observe more data points in region A than in region B, we have evidence for selective publication. That is why it is possible to estimate $p(.)$.*TODO: p() up to scale?*


```{r plot1, echo=FALSE, fig.cap="Identifcation Using Replication Studies",fig.align="center", out.width = '100%'}

knitr::include_graphics("replication.png")

```


### Meta Studies

Using the same set up as in the identification approach using replication studies and adding the crucial assumption that $Σ$ is independent of $Θ$ across the studies, we can take a closer look at Figure 2 which shows the identification approach using meta studies.
Instead of dealing with original and replication estimates and having them on the X and Y axis, we now have the original estimates $X$ on the X axis and the corresponding standard errors $Σ$ on the Y axis.
As before, the left panel shows 100 random draws $(X, Σ)$. Draws which are insignificant on the 5% level are marked as grey $(|X/Σ| ≤ 1.96)$ whereas draws which are significant are drawn blue $(|X/Σ| > 1.96)$.
The right panel shows the draws $(X,Σ)$ which are published. These are exactly the same draws as before but now 80% of the statistically insigfnicant results are deleted because the journal only publishes 20% of the statistically insigifcant results.
Intead of looking at two reagions as in the case of replication studies, we now tae a closer look at two different values of $Σ$, shown in the figure by $A$ and $B$.
Since we have assumed independence between $Σ$ and $Θ$, the distribution of $X$ given greater values of $Σ$ is an inflated version of the same distribution for smaller values of $Σ$. Thus, to the degree that the same is not true for the distribution of the published estimates $X$ given $Σ$, this has to be due to selectivity in the publication process. In this example, statistically insignificant observations are absent for greater values of $Σ$. Since the likelihood of publication is hiher if the result is significant, the estimated values $X$ tend to be larger on average for greater values of $Σ$. This allows for an estimate of $p(.)$.
*TODO: p() up to scale? + Text noch überarbeiten, letzter Teil nicht klar*


```{r plot2, echo=FALSE, fig.cap="Identifcation Using Meta-Studies",fig.align="center", out.width = '100%'}

knitr::include_graphics("meta.png")

```

## Correction

After computing the publication probability, Andrews and Kasy show, that one can derive corrected estimators and confidence sets. Putting it as simple as possible, the selective publication weights the distribution of $Z$, where $Z$ is equal the original estimates divided by their standard errors, by the publication probability. To obtain the corrected estimates and confidence set, one only need to correct for this weighting. 
In their approach, they define the distribution function for published results $Z$ given their *true* effect. By using procedures applied in Andrews (1993) and Stock and Watson (1998), and inverting the distribution function leads to the median unbiased estimator as well as the equal-tailed confidence sets which fully correct for the and bias produced by the selective publication. 
To get an intuition for their result, the illustrative example is continued.
Comparing the median unbiased estimator with the uncorrected estimator, we can clearly see that the median unbiased estimator lies below the original estimator, given $Z$ is small. With an increasing $Z$, we observe that the median unbiased estimator converges to the original estimator. The same observation holds for the confidence intervals. In the example from figure two, the corrected estimates are nearly equal the original estimates is $Z > 5$.

```{r plot3, echo=FALSE, fig.cap="Bias correction", fig.align="center", out.width = '100%'}

knitr::include_graphics("correction.png")

```

# Package Features

## Installing the Package

The package is currently available on GitHub. Running the code below will directly install the package with all its dependencies on your local machine.

```{r, eval = FALSE}

# Installing the package directly from github
devtools::install_github("t-sager/pubias")

# Load package into library
library(pubias)

```


## Syntax

There are two main functions in the package. The first `pubias_meta()` leads to the identification of and correction for the publication bias in meta studies while `pubias_replication` does the same for replication studies. The most simple syntax to get a result is the following:

**Meta Studies**
```{r, eval = FALSE}
pubias_meta(data)

```

**Replication Studies**
```{r, eval = FALSE}
pubias_replication(data)

```


Additionally, there are four functions which are, depending on their specification, called by the two functions above.  The four functions are as follows: `mle_meta()`, `mle_replication()`, `gmm_meta()` and `gmm_replication()`. For example: If the base specification for the meta studies `pubias_meta(data, studynames)` is used, then the function `mle_meta()` is called to calculate the publication probability by MLE. On the other hand, if one executes `pubias_meta(data, studynames GMM = TRUE)` the function `gmm_meta` will be called.
Because the input for these functions heavily depend on he input from the two main function I will not go into further detail about the arguments for these functions. One can also take a closer look at the built in helpfile with `?mle_meta()`.

## Arguments

We have seen that the function works even without specifying more than the data conaining the estimates and the standard errors. Now we will take a closer look at the different arguments which could enter the function. Keep in mind: the arguments for both functions are the same. The only difference is, that in the case of replication studies, the data used has additional columns because we also have to account for the replication estimates.
To check the different arguments, one could also take a closer look at the helpfiles for the two functions with `?pubias_meta()` or `?pubias_replication()`.

The following specifications are currently possible:

```{r, eval = FALSE}

pubias_meta(
  data,
  studynames,
  symmetric = 1,
  sign_lvl = 1.96,
  GMM = FALSE,
  print_plots = FALSE,
  print_dashboard = FALSE
)

```

- `data`: In the case of meta studies the data needs to contain a `n x 3` matrix where the first column contains the original estimates, the second column the associated standard errors and in the third column an ID going from 1 to `n`, where `n` is the number of estimates. If the function for the replication studies is used, the data needs to contain a `n x 4` matrix where the first (third) column contains the standardized original estimates (replication estimates) and the second (fourth) column the associated standard errors , where `n` is the number of estimates.

- `studynames`: This argument is optional. A vector of type character containing all the Studynames of size n in the same order as the argument data.

- `symmetric`: If set to 1, the publication probability is assumed to be symmetric around zero. If set to 0, asymmetry is allowed.

- `sign_lvl`: A value indicating the significance level at which the analysis should be done. Ultimately leads to the threshold (z-score) for the steps of the publication probability. By default, the significance level is set at `5%`, hence `0.05`.

- `GMM`: If set to TRUE, the publication probability will be estimated via GMM. By default, it is set to FALSE which uses the MLE method for estimation.


- `print_plots`: If set to TRUE, descriptive plots as well as correction plots are printed into the working directory in .pdf format.

- `print_dashboard`: If set to TRUE, additionally to the .pdf plots, a dashboard with the same charts in dynamic format is produced. The dashboard is saved in the working directory. Only possbile if print_plots is set to TRUE.


## Results

The output of the functions is pretty simple. They return a list object called `pubias_results`. If only the base specification is run, the list contains two elements:

- `Results`: This element contains the three results `Psihat` which is the estimated publication probability, `Varhat` its variance and`se_robust` containing the robust standard errors for the publication probability. For interpretation: If the function yields `Psihat = 0.25`, the publication of insignificant results is only 25% as likely as the publication of significant results. Or put differently, the publication of a significant result is 4 times more likely than an insignificant result.

- `Corrected Estimates`:  As the name suggests, this element contains the original estimates with their 95% confidence bonds (Z1, Z1_L, Z1_U) as well as the corrected estimates (Z1_M) and in addition the Bonferroni corrected 95% confidence bonds (Z1_LB, Z1_UB). There are additional elements which are mainly used for plotting the results.

If the functions are run with the option to print the plots and dashboard, the result list also contains the `ggplot` objects for the descriptive as well as the correction plots. The plots will additionally be printed as .pdf into the current working directory. If specified, a HTML-Dashboard with dynamic plots will be exported as well. One can also look at the plots by calling for example: 

```{r, eval = FALSE}

pubias_result$`Descriptive Plots`$scatter

```


## Applications

To further illustrate the functionality of the package, we apply it to a replication study as well as one meta analysis. These are the same examples used by Andrews and Kasy (2019).

### Replication Study: Economics Laboratory Experiments

Our first application uses data from a recent large-scale replication of experimental
economics papers by Camerer et al. (2016). The authors replicated all 18
between-subject laboratory experiment papers published in the American Economic
Review and Quarterly Journal of Economics between 2011 and 2014.9 Further details
on the selection and replication of results can be found in Camerer et al. (2016),
while details on our handling of the data are discussed in the online Appendix.
A strength of this dataset for our purposes, beyond the availability of replication
estimates, is the fact that it replicates results from all papers in a particular subfield
published in two leading economics journals over a fixed period of time. This mitigates
concerns about the selection of which studies to replicate. Moreover, since the
authors replicate 18 such studies, it seems likely that they would have published their
results regardless of what they found, consistent with our assumption that selection
operates only on the initial studies and not on the replications.
A caveat to the interpretation of our results is that Camerer et al. (2016) select
the most important statistically significant finding from each paper, as emphasized
by the original authors, for replication. This selection changes the interpretation
of p ( · ) , which has to be interpreted as the probability that a result was published
and selected for replication. In this setting, our corrected estimates and confidence
intervals provide guidance for interpreting the headline results of published studies.
For consistency with the rest of the paper, however, we continue to shorthand p ( · )
as the publication probability.

```{r, eval = FALSE}

# Import data
data <- data_econ

# Has to be matrix
class(data)
dim(data) # 18x4 matrix

# Import Studynames
studynames <- studynames_econ

# Has to be character
class(studynames)

# Apply the main function
pubias_replication(
  data,
  studynames,
  GMM = FALSE,
  symmetric = 1,
  sign_lvl = 0.05,
  print_plots = TRUE,
  print_dashboard = TRUE
)

```

```{r plot4, echo=FALSE, fig.cap="Economics Laboratory Experiments: Descriptives", fig.align="center", out.width = '80%'}

knitr::include_graphics("scatter_hist_econ.pdf")

```


```{r plot5, echo=FALSE, fig.cap="Economics Laboratory Experiments: Original vs. Adjusted Estimates", fig.align="center", out.width = '80%'}

knitr::include_graphics("org_adj_rep_econ.pdf")

```


```{r plot6, echo=FALSE, fig.cap="Economics Laboratory Experiments: Correction Plot", fig.align="center", out.width = '80%'}

knitr::include_graphics("correction_plot_econ.pdf")

```


### Meta Analysis: Deworming

This application uses a meta-study by Croke et al. (2016). They look at the effect of drug for deworming on the body weight of children. The authors collect a total of 22 estimates frm 20 different studies. These 22 estimates together with their associated standard errors build the main data input for our function `pubias_meta()`.
As stated before, the data for the two documented applications can be loaded directly from the package as shown below. Of course, alternatively, one can load own estimates and studynames via `read.csv()` or similar functions.

```{r, eval = FALSE}

# Import data
data <- data_dew

# Has to be matrix
class(data)
dim(data) # 22x3 matrix

# Import Studynames
studynames <- studynames_dew

# Has to be character
class(studynames)

# Apply the main function
pubias_meta(
  data,
  studynames,
  GMM = TRUE,
  symmetric = 1,
  sign_lvl = 0.05,
  print_plots = TRUE,
  print_dashboard = TRUE
)

```

```{r plot7, echo=FALSE, fig.cap="Deworming: Descriptives", fig.align="center", out.width = '80%'}

knitr::include_graphics("scatter_hist_dew.pdf")

```


```{r plot8, echo=FALSE, fig.cap="Deworming: Original vs. Adjusted Estimates", fig.align="center", out.width = '80%'}

knitr::include_graphics("org_adj_dew.pdf")

```


```{r plot9, echo=FALSE, fig.cap="Deworming: Correction Plot", fig.align="center", out.width = '80%'}

knitr::include_graphics("correction_plot_dew.pdf")

```

# Final remarks


# Literature
